---
layout: reveal
type: teaching
title: Creative Prediction Projects
permalink: /presentations/creprepro/
theme: assets/revealthemes/crepre.scss
---

{% include slides/title.html %}

## Supervisors

**Charles Martin**. Lecturer in Computer Science, Australian
National University. charles.martin@anu.edu.au



**Benedikte Wallace**. PhD Researcher, University of Oslo.
benediwa@ifi.uio.no

{:style="width:30%;float:right"}
![]({{site.baseurl}}/assets/people/charlesmartin.jpg)

{:style="width:30%;float:right"}
![]({{site.baseurl}}/assets/people/benediktewallace.jpg)





## Creative Predictions

{:style="width:100%}
![]({{site.baseurl}}/assets/creative-prediction-image.png)


## Learning to Predict Sequences

{:style="width:100%"}
![]({{site.baseurl}}/assets/sequence-learning.png)


## Melody to Harmony in MicroJam

{:style="float:right;"}
![]({{site.baseurl}}/assets/robojam-interaction.png)

1. Gain an overview of DL for music generation.
2. Develop a melody to harmony sequence to sequence model
3. Train the model on matched melody/harmony sequences 
4. Use MicroJam-sourced data as input and see if the generated harmonies make sense!


## Seq-to-Seq Music Generation

{:style="width:75%"}
![](https://magenta.tensorflow.org/assets/music_transformer/motifs_shaded_boxes.png)

1. Understand the Transformer architecture.
2. Implement your own Transformer (e.g., in Keras).
3. Find a musical dataset that could be trained.
4. Train your model, listen to the results and find a way to evaluate them.


## Generating colour palettes from audio data

{:style="width:50%"}
![]({{site.baseurl}}/assets/nainoa-shizuru-NcdG9mK3PBY-unsplash.jpg)

1. Gain an overview of DL for audio processing.
2. Obtain a dataset of audio and video (or colour) data.
3. Try different neural network designs and evaluate the results. (Even a simple fully-connected ANN might work well!)


## Motion-to-Motion Generators

{:style="width:80%"}
![]({{site.baseurl}}/assets/motion-to-motion.png)

1. Gain an overview of the main DL methods used for motion generation including RNNs, MDRNNs, and world models.
2. Find a dataset of motion capture or other movement data (or capture one yourself!)
3. Train the ANN and evaluate its generative abilities.

