---
layout: reveal
title:  Deep Learning in the Cloud
---

<section data-markdown>
    <textarea data-template>
        # Deep Learning in the Cloud

        Charles P. Martin 2018
    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
        ## Why cloud for ML/AI?

        - Not always convenient/cost effective to use big workstation.
        - We like small laptops without hot GPUs and processors.
        - We might want to move from research to product!
        - The internet is cool/fun?
    </textarea>
</section>

<section data-markdown>
<textarea data-template>
![Cloud Models]({{site.baseurl}}/assets/dl-on-cloud/cloud-models.png) <!-- .element: width="80%" -->

[src: www.comgt.com/lib/sw/deliverymodels/](http://www.comgt.com/lib/sw/deliverymodels/)
</textarea>
</section>

<section data-markdown>
<textarea data-template>
![Pizza-as-a-Service]({{site.baseurl}}/assets/dl-on-cloud/pizza-as-a-service.jpg) <!-- .element: width="60%" -->
  
[src: Albert Barron](https://www.linkedin.com/pulse/20140730172610-9679881-pizza-as-a-service/)
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
        ## What do we need?

        - GPUs: Nvidia [GTX1080TI, K80, P100, V100] or... "Tensor Processing Units"
        - OS: Linux?
        - CUDA + CUDnn
        - Python
        - Python libraries: Tensorflow, Keras, SKLearn, etc.
        - Jupyter 
    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
        ## On Premises: Workstations

        <img align="right" width="25%" src="{{site.baseurl}}/assets/dl-on-cloud/workstation.jpg">

        - Workstations (15-50KNOK)
        - Pro: fun to play with
        - Pro: good for small number of users
        - Pro: one-time cost
        - Con: not practical for many users
        - Con: have to keep setting up eduroam
        - Con: I don't like sharing?
    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
        ## Infrastructure-as-a-Service

        <img align="right" width="25%" src="{{site.baseurl}}/assets/dl-on-cloud/cloud-server.png">

        - Virtual servers
        - Set up server, access via Linux shell
        - Amazon Web Services (AWS)
        - Google Cloud Platform (GCP)
        - DigitalOcean (DO)
        - [UH Cloud](http://www.uh-iaas.no) (UiO)
    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
        ## Platform-as-a-Service

        <img align="center" width="25%" src="{{site.baseurl}}/assets/dl-on-cloud/cloud-dashboard.png">

        - Google Cloud Kubenetes Engine
        - Deploy "Containerised" application to servers.
        - ([Deploy DL to Kubernetes](https://medium.com/analytics-vidhya/deploy-your-first-deep-learning-model-on-kubernetes-with-python-keras-flask-and-docker-575dc07d9e76))
        - [Sigma2](https://www.sigma2.no) (UiO)
    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
        ## Software-as-a-Service

        <img align="right" width="25%" src="{{site.baseurl}}/assets/dl-on-cloud/colab.png">

        - Google Colaboratory (üëèüèº)
        - Kaggle Kernels
    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
        # Example: Robojam

        <img align="center" width="20%" src="{{site.baseurl}}/assets/dl-on-cloud/microjam-iphone8-robojam.png">

    </textarea>
</section>


<section data-markdown>
    <textarea data-template>
        ## Example: Robojam

        <img align="center" width="75%" src="{{site.baseurl}}/assets/dl-on-cloud/robojam-application.png">

        RoboJam is a Keras project, now deployed as a Flask web application.
    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
        ## Starting point: Local + SaaS

        - Developed on local shared workstation
        - Also worked on Colaboratory
        - Tips:
            - keep jupyter sessions around with `screen`
            - tunnel jupyter port with `ssh -L 8888:localhost:8888`
        - Could also use Google Cloud VMs with GPUs for short training runs
    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
        ## Starting point: IaaS + PaaS

        <img align="center" width="30%" src="{{site.baseurl}}/assets/dl-on-cloud/deep-learning-vm-image.png">

        - Can use VMs and Containers for DL development
        - [Google's "Cloud Deep Learning VM Image"](https://cloud.google.com/deep-learning-vm/)
        - Comes with `jupyterhub` running to do development in a browser.
        - Expensive for a good machine: K80 GPU 0.45USD/h
    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
        ## SaaS Architecture on Colab

        ![]({{site.baseurl}}/assets/dl-on-cloud/robojam-cloud-architecture-colab.png)
    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
        ## Production: Turning into a web service

        - Used `flask` framework to create a RESTful web API
        - Just one endpoint: `https://0.0.0.0:5000/api/predict`
        - Send performance as JSON to that endpoit
        - robojam RNN model is conditioned with input, then a continuation is predicted.
        - prediction returned as JSON
        - "[Deploying DL models with Flask](https://towardsdatascience.com/deploying-keras-deep-learning-models-with-flask-5da4181436a2)"
    </textarea>
</section>



<section data-markdown>
    <textarea data-template>
        ## Production: Deploying to DigitalOcean

        - Using cheapest DigitalOcean VM: 1vCPU, 1GB, $5 per month.
        - Login, clone git repo, run server in a detached `screen`.
        - Works! Deployed for about a year.
        - Predictions take about 1.0s-1.5s, not too bad.
        - Problem: what if the app gets popular?
    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
        ## IaaS Architecture on DigitalOcean

        ![]({{site.baseurl}}/assets/dl-on-cloud/robojam-cloud-architecture-vm.png)
    </textarea>
</section>


<section data-markdown>
    <textarea data-template>
        ## Containerising: Docker

        - We want to make a "container" that includes Robojam and all necessary libraries to run on any Docker installation.
        - We'll start with the tensorflow docker which includes a development environment for tensorflow. 
    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
        ## Containerising: Dockerfile

            FROM tensorflow/tensorflow:latest-py3
            MAINTAINER Charles Martin "charlepm@ifi.uio.no"

            COPY requirements.txt /tmp/
            RUN pip install --requirement /tmp/requirements.txt
            COPY . /tmp/
            WORKDIR /tmp
            CMD [ "python", "./serve_tiny_performance_mdrnn.py" ]

    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
        ## Containerising: Building the container

            sudo docker build -t robojam:latest .
            docker tag robojam:latest charlepm/robojam:latest
            docker push charlepm/robojam:latest
    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
        ## Containerising: Running the Container

            docker run -d -p 5000:5000 robojam:latest

    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
        ## Containerised Architecture with Docker

        ![]({{site.baseurl}}/assets/dl-on-cloud/robojam-cloud-architecture-docker.png)

    </textarea>
</section>


<section data-markdown>
    <textarea data-template>
        ## Deploying to Kubenetes

        - Kubernetes is a system to run docker containers on multiple computers simultaneously.
        - Let's set up a little cluster on Google Cloud Platform and deploy Robojam.
        - Need to set up computers through the web interface
        - Then use command interface to start Robojam.
    </textarea>
</section>       

<section data-markdown>
    <textarea data-template>
        ## Deploying Robojam to Cluster

            kubectl run robojam-cluster --image=charlepm/robojam:latest --port 5000
            kubectl get pods
            kubectl expose deployment robojam-cluster --type=LoadBalancer --port 5000 --target-port 5000
            kubectl get service

    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
        ## Micro-Service Architecture with Kubernetes

        ![]({{site.baseurl}}/assets/dl-on-cloud/robojam-cloud-architecture-kubenetes.png)

    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
        ## Conclusion

        - ML/AI isn't just for research, we can make cool applications too!
        - Cloud resources very available:
            - Not too expensive to use powerful servers for short time (training)
            - can do a lot with cheap servers for production
        - Try out docker etc, makes life much easier.
        - Try out Jupyterhub for development. Might be way of future?
    </textarea>
</section>

<section data-markdown>
    <textarea data-template>
        ## Extra links

        - [Maximise your GPU Dollars](https://towardsdatascience.com/maximize-your-gpu-dollars-a9133f4e546a)

    </textarea>
</section>
