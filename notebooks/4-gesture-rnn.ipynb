{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture RNN - predicting multiple sequences!\n",
    "\n",
    "The idea with this RNN is to help build a system that can mimic ensemble performances.\n",
    "\n",
    "![](figures/gesture-rnn-neural-mode-ensemble.jpg)\n",
    "\n",
    "In these performances, the touchscreen gestures were classified once every second (by a different ML system). This means that each performance can be represented as a sequence of gestures for each performer.\n",
    "\n",
    "Let's load in the gesture data and see what these look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from itertools import permutations\n",
    "\n",
    "import urllib.request\n",
    "URL = \"https://github.com/anucc/metatone-analysis/raw/master/metadata/metatone_performances_dataframe.h5\"\n",
    "PICKLE_FILE = \"metatone_performances_dataframe.h5\"\n",
    "\n",
    "if not os.path.exists(PICKLE_FILE):\n",
    "    urllib.request.urlretrieve(URL, PICKLE_FILE)\n",
    "metatone_dataset = pd.read_hdf(PICKLE_FILE)\n",
    "\n",
    "## Int values for Gesture codes.\n",
    "NUMBER_GESTURES = 9\n",
    "GESTURE_CODES = {\n",
    "    'N': 0,\n",
    "    'FT': 1,\n",
    "    'ST': 2,\n",
    "    'FS': 3,\n",
    "    'FSA': 4,\n",
    "    'VSS': 5,\n",
    "    'BS': 6,\n",
    "    'SS': 7,\n",
    "    'C': 8}\n",
    "\n",
    "vocabulary_size = len(GESTURE_CODES)\n",
    "\n",
    "print(\"Here's an example of some gestures from a trio performance:\")\n",
    "metatone_dataset.gestures.iloc[0][:20].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and Decoding multiple numbers\n",
    "\n",
    "- We can encode and decode multiple integer into a single one\n",
    "- Good trick! We can then use a simple CharRNN to predict more than one sequence!\n",
    "\n",
    "E.g., if you have some numbers:\n",
    "$$ g_1,g_2,\\ldots,g_n \\in [1,j-1]$$\n",
    "then these can be encoded as one number:\n",
    "$$g_1j^0 + g_2j^1 + \\ldots + g_nj^{(n-1)}$$\n",
    "and decoded again preserving the ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_ensemble_gestures(gestures):\n",
    "    \"\"\"Encode multiple natural numbers into one\"\"\"\n",
    "    encoded = 0\n",
    "    for i, g in enumerate(gestures):\n",
    "        encoded += g * (len(GESTURE_CODES) ** i)\n",
    "    return encoded\n",
    "\n",
    "def decode_ensemble_gestures(num_perfs, code):\n",
    "    \"\"\"Decodes ensemble gestures from a single int\"\"\"\n",
    "    gestures = []\n",
    "    for i in range(num_perfs):\n",
    "        part = code % (len(GESTURE_CODES) ** (i + 1))\n",
    "        gestures.append(part // (len(GESTURE_CODES) ** i))\n",
    "    return gestures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just check that this works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gestures = [0,5,2,7]\n",
    "enc_gestures = encode_ensemble_gestures(gestures)\n",
    "print(\"Starting gestures are:\", gestures)\n",
    "print(\"Encoded integer is:\", enc_gestures)\n",
    "print(\"Decoded gestures are:\", decode_ensemble_gestures(4, enc_gestures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking only at Quartet Performances\n",
    "\n",
    "- Gesture-RNN is mainly used for quartets (one live player and three RNN-controller players)\n",
    "- Let's extract the quartets from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Isolate the Interesting Ensemble Performances\n",
    "improvisations = metatone_dataset[\n",
    "    (metatone_dataset[\"performance_type\"] == \"improvisation\") &\n",
    "    (metatone_dataset[\"performance_context\"] != \"demonstration\") &\n",
    "    (metatone_dataset[\"number_performers\"] == 4)]\n",
    "gesture_data = improvisations['gestures']\n",
    "#metatone_dataset[\"number_performers\"]\n",
    "print(\"Number of performances for testing: \", len(improvisations))\n",
    "\n",
    "\n",
    "num_input_performers = 4\n",
    "num_output_performers = 3\n",
    "\n",
    "ensemble_improvisations = gesture_data.tolist()\n",
    "\n",
    "print(\"Here's an example of part of one performance:\")\n",
    "ensemble_improvisations[0][:20].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the training data\n",
    "\n",
    "- This code cuts up the performances into training examples.\n",
    "- One player is taken as lead and then other players are permuted to extend the data.\n",
    "- Take a bit of a long time to run this code (a bit inefficient?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup the epochs\n",
    "## Each batch is of single gestures as input and tuples of remaining performers as output\n",
    "## Setup the inputs and label sets\n",
    "num_steps = 30\n",
    "\n",
    "imp_xs = []\n",
    "imp_ys = []\n",
    "    \n",
    "for n, imp in enumerate(ensemble_improvisations):\n",
    "    print(\"Processing sequence:\", n)\n",
    "    for i in range(len(imp)-num_steps-1):\n",
    "        imp_slice = imp[i:i+num_steps+1]\n",
    "        for j in range(len(imp_slice.T)):\n",
    "            lead = imp_slice[1:].T[j] # lead gestures (post steps)\n",
    "            ensemble = imp_slice.T[np.arange(len(imp_slice.T)) != j] # rest of the players indexed by player\n",
    "            for ens_perm in permutations(ensemble): # consider all permutations of the players\n",
    "                ens_pre = np.array(ens_perm).T[:-1] # indexed by time slice\n",
    "                #ens_post = np.array(ens_perm).T[1:] # indexed by time slice\n",
    "                ens_post = np.array(ens_perm).T[-1] # indexed by time slice\n",
    "                y = encode_ensemble_gestures(ens_post)\n",
    "                #y = [encode_ensemble_gestures(e) for e in ens_post]\n",
    "                #y = ens_post # test just show the gestures\n",
    "                x = [encode_ensemble_gestures(e) for e in zip(lead,*(ens_pre.T))] # encode ensemble state\n",
    "                #x = zip(lead,*(ens_pre.T)) # test just show the gestures\n",
    "                imp_xs.append(x) # append the inputs\n",
    "                imp_ys.append(y) # append the outputs\n",
    "\n",
    "print(\"Total Training Examples: \" + str(len(imp_xs)))\n",
    "print(\"Total Training Labels: \" + str(len(imp_ys)))\n",
    "\n",
    "X = np.array(imp_xs, dtype=np.int16)\n",
    "y = np.array(imp_ys, dtype=np.int16)\n",
    "\n",
    "print(\"X:\", X.shape)\n",
    "print(\"y:\", y.shape)\n",
    "\n",
    "np.savez('../datasets/gesture_rnn_training_dataset.npz', X=X.astype(np.int16), y=y.astype(np.int16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here's one I prepared earlier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load(\"../datasets/gesture_rnn_training_dataset.npz\")\n",
    "X = dataset['X']\n",
    "y = dataset['y'].reshape(-1,1)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X:\", X[100])\n",
    "print(\"y:\", y[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buiding the RNN\n",
    "\n",
    "- Very similar design to the CharRNN.\n",
    "- Gestures are combined into one integer before entering RNN.\n",
    "- RNN outputs one integer which is split up.\n",
    "- Input size is different to output! 4 --> 3\n",
    "\n",
    "![](figures/gesture-rnn-training.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "num_layers = 1\n",
    "batch_size = 64\n",
    "num_units = 256\n",
    "num_steps = 120\n",
    "\n",
    "num_input_performers = 4\n",
    "num_output_performers = 3\n",
    "\n",
    "vocabulary_size = 9 # len(GESTURE_CODES)\n",
    "num_input_classes = vocabulary_size ** num_input_performers\n",
    "num_output_classes = vocabulary_size ** num_output_performers\n",
    "\n",
    "training_model = keras.models.Sequential()\n",
    "training_model.add(keras.Input(shape=(num_input_classes,)))\n",
    "training_model.add(keras.layers.Embedding(num_input_classes, num_units))\n",
    "for n in range(num_layers - 1):\n",
    "    training_model.add(keras.layers.LSTM(num_units, return_sequences=True))\n",
    "training_model.add(keras.layers.LSTM(num_units))\n",
    "training_model.add(keras.layers.Dense(num_output_classes, activation='softmax'))\n",
    "\n",
    "training_model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam')\n",
    "training_model.summary()\n",
    "\n",
    "# Notes:\n",
    "# Difficulty of this task is learning from a relatively large input space:\n",
    "print(\"Number of input classes:\", num_input_classes)\n",
    "print(\"Number of output classes\", num_output_classes)\n",
    "# It's handy to use an Embedding layer so that we can learn from integer\n",
    "# inputs (not one-hot)\n",
    "# This means that for lower 'num_units', the parameters used for the input \n",
    "# embedding outnumber the LSTM layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- training takes a long time for this RNN! (long sequence length, lots of example)\n",
    "- 9 hours for a 3-layer 512 unit network.\n",
    "- not going to do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Model.\n",
    "training_model.fit(X, y, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "\n",
    "However, the trained model works! Here's an example of a generated score (red is the lead player) and a demo performance.\n",
    "\n",
    "![](figures/gesture_rnn_4to3_example.png)\n",
    "\n",
    "![](figures/gesture-rnn-band-demo.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
