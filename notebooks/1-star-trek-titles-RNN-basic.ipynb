{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Star Trek Titles with an RNN\n",
    "\n",
    "- Trains on Star Trek episode titles\n",
    "- Outputs \"fake\" titles.\n",
    "- Uses the \"charRNN\" idea.\n",
    "\n",
    "Much of this example is borrowed from Fran√ßois Chollet's [Keras examples](https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py).\n",
    "\n",
    "This notebook was developed by Charles Martin and Kai Olav Ellefsen at the University of Oslo, Department of Informatics.\n",
    "\n",
    "## Setup Environment\n",
    "\n",
    "- Import Keras\n",
    "- Open up the Star Trek corpus\n",
    "- We need to translate the textual data into a format that the RNN can accept as input.\n",
    "- Give each letter an index and create dictionaries to translate from index to character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Much borrowed from https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Jupyter Notebook Only\n",
    "# text = open(\"../datasets/startrekepisodes.txt\").read().lower()\n",
    "# print('corpus length:', len(text))\n",
    "\n",
    "# Colab\n",
    "ST_EPISODES_URL = \"https://raw.githubusercontent.com/cpmpercussion/creative-prediction/master/datasets/startrekepisodes.txt\"\n",
    "text = urlopen(ST_EPISODES_URL).read().decode(\"utf-8\").lower()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocabulary_size = len(chars)\n",
    "print('total chars:', vocabulary_size)\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "\n",
    "# How long is a title?\n",
    "titles = text.split('\\n')\n",
    "lengths = np.array([len(n) for n in titles])\n",
    "print(\"Max:\", np.max(lengths))\n",
    "print(\"Mean:\", np.mean(lengths))\n",
    "print(\"Median:\", np.median(lengths))\n",
    "print(\"Min:\", np.min(lengths))\n",
    "print()\n",
    "\n",
    "# hence choose 30 as sequence length to train on.\n",
    "print(\"Character Dictionary: \", char_indices, \"\\n\")\n",
    "print(\"Inverse Character Dictionary: \", indices_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training Data\n",
    "\n",
    "- Cut up the corpus into semi-redundant sequences of 30 characters.\n",
    "- Change indices into \"one-hot\" vector encodings.\n",
    "\n",
    "<img src=\"figures/slicing_text.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 30\n",
    "step = 3\n",
    "\n",
    "sentences = [] #The training data\n",
    "next_chars = [] #The training labels\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "    \n",
    "print('Number of sequences (Xs):', len(sentences))\n",
    "print('Number of next_chars (ys):', len(next_chars))\n",
    "\n",
    "print(\"\\nHere's the first example:\")\n",
    "print(\"X:\",sentences[0])\n",
    "print(\"y:\",next_chars[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onehot encoding:\n",
    "\n",
    "* `a -> [1, 0, 0, ..., 0]`\n",
    "* `b -> [0, 1, 0, ..., 0]`\n",
    "* ...\n",
    "\n",
    "Each training sample becomes 2D tensor:\n",
    "\n",
    "* `\"This is the text\" -> X = [[0, 0, ..., 1, 0, ..., 0], ..., [0, 0, ..., 1, 0, ... 0]]`\n",
    "\n",
    "Each target (next letter) becomes 1D onehot tensor:\n",
    "\n",
    "* `a -> y = [1, 0, 0, ..., 0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X shape: 3D tensor. First dimension is the sentences, second is each letter in each sentence, third is the onehot\n",
    "#vector representing that letter.\n",
    "X = np.zeros((len(sentences), maxlen, vocabulary_size), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), vocabulary_size), dtype=np.bool)\n",
    "    \n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "    \n",
    "print(\"Done preparing training corpus, shapes of sets are:\")\n",
    "print(\"X shape: \" + str(X.shape))\n",
    "print(\"y shape: \" + str(y.shape))\n",
    "print(\"Vocabulary of characters:\", vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some data:\n",
    "print(X[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "- Model has one hidden layer of 128 LSTM cells.\n",
    "- Output layer uses the \"softmax\" activation function to output a probability distribution over next letters.\n",
    "\n",
    "This is model is designed for \"one-by-one\" prediction, i.e., it predicts the very next letter in a sequence of text. \n",
    "\n",
    "- For the sentence \"My cat is named Simon\"\n",
    "   - x: \"My cat is named Simo\"\n",
    "   - y: \"n\"\n",
    "   \n",
    "The RNN is structured as follows:\n",
    "\n",
    "<img src=\"figures/n-in-1-out.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layer_size = 128\n",
    "# build the model: a single LSTM layer.\n",
    "model_train = Sequential()\n",
    "\n",
    "model_train.add(LSTM(layer_size, input_shape=(maxlen, len(chars))))\n",
    "# Project back to vocabulary. One output node for each letter.\n",
    "# Dense indicates a fully connected layer.\n",
    "# Softmax activation ensures the combined values of all outputs form a probability distribution:\n",
    "# They sum to 1, with each individual value between 0 and 1.\n",
    "model_train.add(Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical crossentropy  minimizes the distance between the probability distributions \n",
    "# output by the network and the true distribution of the targets.\n",
    "# The optimizer specifies HOW the gradient of the loss will be used to update parameters.\n",
    "# Different optimizers have different tricks to avoid local optima, etc.\n",
    "# RMSProp is adaptive, adjusting the rate of learning to how fast we're currently learning.\n",
    "# Choose one by experimenting, or selecting one documented to work well for this problem by other researchers.\n",
    "model_train.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01))\n",
    "model_train.summary()\n",
    "\n",
    "# LSTM is more complicated than the basic RNN we introduced. It has more free parameters, therefore more parameters \n",
    "# than one might expect below. We use them since they are better at learning long-term structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the Model\n",
    "\n",
    "- The model doesn't output _letters_, but a distribution for the probability for each letter. - Could just take letter with max probability\n",
    "- Better to do a random sampling from the distribution.\n",
    "- Also have opportunity to \"reweight\" the distribution, to make more \"creative\" choices.\n",
    "\n",
    "<img src=\"figures/reweighting.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "- Here's the code for the sampling function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Higher diversity -> more randomness in the generation.\n",
    "def sample(probability_distribution, diversity=1.0):\n",
    "    # helper function to sample an index from a probability distribution\n",
    "    probability_distribution = np.asarray(probability_distribution).astype('float64')\n",
    "    # Reweight the distribution\n",
    "    probability_distribution = np.log(probability_distribution) / diversity\n",
    "    # Here's the Softmax operation\n",
    "    exp_preds = np.exp(probability_distribution)\n",
    "    probability_distribution = exp_preds / np.sum(exp_preds)\n",
    "    #Draws 1 element at random according to the new scaled probability-distribution.\n",
    "    probabilities = np.random.multinomial(n=1, pvals = probability_distribution) \n",
    "    return np.argmax(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for printing some example text after every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_segment(length, diversity, generating_model = model_train, input_sequence_length = maxlen):\n",
    "    start_index = random.randint(0, len(text) - input_sequence_length - 1)\n",
    "\n",
    "    # We need a seed to start the text generation. Since during training the ANN always experiences\n",
    "    # sentences of size 30, we seed it with a sentence of length 30 to get it into a sensible state.\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + input_sequence_length]\n",
    "    generated += sentence\n",
    "    \n",
    "    for i in range(length):\n",
    "        x_pred = np.zeros((1, input_sequence_length, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "        \n",
    "\n",
    "        predictions_distribution = generating_model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(predictions_distribution, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        #Stepping one symbol forward in the sentence\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "    return generated\n",
    "\n",
    "def generate_sample_text(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    generated = generate_text_segment(200, 1.0, model_train, input_sequence_length = maxlen)\n",
    "    print(\"Seed:\\n\", generated[:30], \"\\n\")\n",
    "    print(\"Generated text:\\n\", generated[30:], \"\\n\\n\")\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=generate_sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- Training in Keras is done by calling `model_train.fit(X,y)`, where `X`, `y` are the data corpus we prepared earlier.\n",
    "- There's two important paramters for training:\n",
    "    - Batch size: How many examples are used to make one weight update in the model.\n",
    "    - Number of epochs: How many times to iterate through the whole dataset (randomised batches each time).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model_train.fit(X, y, batch_size=128, epochs=50, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model if necessary\n",
    "model_train.save(\"keras-startrek-LSTM-model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting training and validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'b-', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Decoder model\n",
    "\n",
    "During training, we presented sequences of 30 characters, along with the correct next character.\n",
    "When_using the trained model, it may be more useful to feed in 1 character at a time, and seeing the next\n",
    "predicted one. That will also convince us that the network is actually _using_ its internal state.\n",
    "\n",
    "- Needs input length of 1.\n",
    "- Needs batch size of 1\n",
    "- Needs LSTM to be stateful\n",
    "- check that params is the same as model_train\n",
    "\n",
    "<img src=\"figures/1-in-1-out.png\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model if necessary.\n",
    "# model_train = load_model(\"keras-startrek-LSTM-model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a decoding model (input length 1, batch size 1, stateful)\n",
    "layer_size = 128\n",
    "\n",
    "model_dec = Sequential()\n",
    "# 1 letter in, 1 letter out.\n",
    "# Stateful=True keeps the state from the end of one batch to the start of the next\n",
    "# In other words, the network \"remembers\" its state from one input to the next. This is essential when\n",
    "# the network looks at 1 input at a time.\n",
    "model_dec.add(LSTM(layer_size, stateful=True, batch_input_shape=(1,1,len(chars))))\n",
    "\n",
    "# project back to vocabulary\n",
    "model_dec.add(Dense(vocabulary_size, activation='softmax'))\n",
    "model_dec.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01))\n",
    "model_dec.summary()\n",
    "\n",
    "# set weights from training model\n",
    "# Note that we can reuse these weights, since the sizes of the trained and decoder network are the same.\n",
    "# The trained network took in 30 characters, but remember that all these 30 used the same input weights.\n",
    "# That is one of the advantages of RNNs: They are independent of sequence lengths.\n",
    "model_dec.set_weights(model_train.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Model\n",
    "\n",
    "- Take a quote then add 400 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 1000 characters from the decoding model using a random seed from the vocabulary.\n",
    "generated = generate_text_segment(1000, diversity=1.0, generating_model = model_dec, input_sequence_length = 1)\n",
    "sys.stdout.write(generated)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {
    "68a0903c8cb44d7f844576a9e191a074": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "73196c60cb1f4fa1a36a9200fb5ec526": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    },
    "75ed5e35093c4569af70d449c8d599dd": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "aa7bb86c093a47a48ddd69380fe4d1b1": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
